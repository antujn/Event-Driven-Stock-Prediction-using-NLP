{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional, GRU, BatchNormalization\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = pd.read_pickle('reuters_news_concatenated.pkl', 'bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize News into a 2D integer Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 512\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(reuters.news)\n",
    "\n",
    "reuters_sequences = tokenizer.texts_to_sequences(reuters.news)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "reuters_sequences = pad_sequences(reuters_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - Strong Sell\n",
    "1 - Sell\n",
    "2 - Buy\n",
    "3 - Strong Buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 4\n",
    "labels = np.copy(reuters.Y)\n",
    "for i in range(1, clusters):\n",
    "    print(np.percentile(reuters.Y, 100*i/clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 4\n",
    "labels = np.copy(reuters.Y)\n",
    "labels[reuters.Y<np.percentile(reuters.Y, 100/clusters)] = 0\n",
    "for i in range(1, clusters):\n",
    "    labels[reuters.Y>np.percentile(reuters.Y, 100*i/clusters)] = i\n",
    "reuters.Y = labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(reuters.Y, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_labels = to_categorical(reuters.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - Sell\n",
    "1 - Buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters = 2\n",
    "# labels = np.copy(reuters.Y)\n",
    "# for i in range(1, clusters):\n",
    "#     print(np.percentile(reuters.Y, 100*i/clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters = 2\n",
    "# labels = np.copy(reuters.Y)\n",
    "# labels[reuters.Y<np.percentile(reuters.Y, 100/clusters)] = 0\n",
    "# for i in range(1, clusters):\n",
    "#     labels[reuters.Y>np.percentile(reuters.Y, 100*i/clusters)] = i\n",
    "# reuters['binY'] = labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique, counts = np.unique(reuters.binY, return_counts=True)\n",
    "# print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuters_bin_labels = to_categorical(reuters.binY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuters_bin_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of news tensor:', reuters_sequences.shape)\n",
    "print('Shape of label tensor:', reuters_labels.shape)\n",
    "# print('Shape of binary label tensor:', reuters_bin_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(reuters)) < 0.8\n",
    "\n",
    "train_X = reuters_sequences[mask]\n",
    "train_Y = reuters_labels[mask]\n",
    "# train_binY = reuters_bin_labels[mask]\n",
    "val_X = reuters_sequences[~mask]\n",
    "val_Y = reuters_labels[~mask]\n",
    "# val_binY = reuters_bin_labels[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "if (not os.path.isfile('glove.42B.300d.zip') and\n",
    "   not os.path.isfile('glove.42B.300d.txt')):\n",
    "    urllib.request.urlretrieve('http://nlp.stanford.edu/data/glove.42B.300d.zip', \n",
    "                              os.path.join(os.getcwd(), 'glove.42B.300d.zip'))\n",
    "\n",
    "import zipfile\n",
    "if not os.path.isfile('glove.42B.300d.txt'):\n",
    "    with zipfile.ZipFile(\"glove.42B.300d.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(os.getcwd())\n",
    "\n",
    "glove_wordmap = {}\n",
    "with open('glove.42B.300d.txt', \"r\", encoding='utf8') as glove:\n",
    "    for line in glove:\n",
    "        word, vector = tuple(line.split(\" \", 1))\n",
    "        glove_wordmap[word] = np.fromstring(vector, sep=\" \")\n",
    "\n",
    "# def sentence2sequence(tokens):\n",
    "#     global glove_wordmap\n",
    "   \n",
    "#     feature = np.zeros([0, 300])\n",
    "#     for token in tokens:\n",
    "#         try:\n",
    "#             feature = np.vstack((feature, glove_wordmap[token]))\n",
    "#         except:\n",
    "#             pass\n",
    "   \n",
    "#     return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index), 300))\n",
    "count=0\n",
    "for word, i in word_index.items():\n",
    "    word_vector = glove_wordmap.get(word)\n",
    "    if word_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        try:\n",
    "            embedding_matrix[i] = word_vector\n",
    "        except:\n",
    "            pass\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count/len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(MAX_NUM_WORDS,\n",
    "                            300,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Architectue (1D convnet with global maxpooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_1D_GMP(clusters=4):\n",
    "    model = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    model = MaxPooling1D(5)(model)\n",
    "#     model = Dropout(0.2)(model)\n",
    "    model = Conv1D(128, 5, activation='relu')(model)\n",
    "    model = MaxPooling1D(5)(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Conv1D(128, 5, activation='relu')(model)\n",
    "    model = GlobalMaxPooling1D()(model)\n",
    "#     model = Dropout(0.2)(model)\n",
    "    model = Dense(128, activation='relu')(model)\n",
    "\n",
    "    model = Model(sequence_input, Dense(clusters, activation='softmax')(model))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Architectue (Bidirectional GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BI_DIR_GRU(clusters=4):\n",
    "    model = Bidirectional(GRU(128, return_sequences=True, activation='relu'))(embedded_sequences)\n",
    "    model = Bidirectional(GRU(128, return_sequences=True, activation='relu'))(model)\n",
    "    model = BatchNormalization(axis=-1)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(128,activation='relu')(model)\n",
    "    model = BatchNormalization(axis=-1)(model)\n",
    "#     model = Dropout(0.2)(model)\n",
    "#     model = Dense(128,activation='relu')(model)\n",
    "#     model = BatchNormalization(axis=-1)(model)\n",
    "\n",
    "    model = Model(sequence_input, Dense(clusters, activation='softmax')(model))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN_1D_GMP_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN_1D_GMP(clusters=2)\n",
    "# model = load_model('CNN_1D_GMP_bin.h5')\n",
    "# model.fit(train_X, train_binY,\n",
    "#           batch_size=128,\n",
    "#           epochs=12,\n",
    "#           validation_data=(val_X, val_binY))\n",
    "# model.save('CNN_1D_GMP_bin.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate CNN_1D_GMP_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = np.argmax(model.predict(val_X), axis=-1)\n",
    "# conf = confusion_matrix(np.argmax(val_binY, axis=-1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(val_binY, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(conf,\n",
    "#              index = [i for i in ['Sell', 'Buy']],\n",
    "#              columns = [i for i in ['Sell', 'Buy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conf = np.array(conf)\n",
    "# for i in range(2):\n",
    "#     print(\"Label %d Precision: %.2f%%\" % (i, conf[i,i] * 100.0 / sum(conf[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_ml import ConfusionMatrix\n",
    "# ConfusionMatrix(np.argmax(val_Y, axis=-1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matthews_corrcoef(np.argmax(val_binY, axis=-1), predictions)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN_1D_GMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = CNN_1D_GMP(clusters=4)\n",
    "model = load_model('CNN_1D_GMP.h5')\n",
    "model.fit(train_X, train_Y,\n",
    "          batch_size=128,\n",
    "          epochs=6,\n",
    "          validation_data=(val_X, val_Y))\n",
    "model.save('CNN_1D_GMP.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate CNN_1D_GMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(model.predict(val_X), axis=-1)\n",
    "conf = confusion_matrix(np.argmax(val_Y, axis=-1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(val_Y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf,\n",
    "             index = [i for i in ['Strong Sell', 'Sell', 'Buy', 'Strong Buy'] ],\n",
    "             columns = [i for i in ['Strong Sell', 'Sell', 'Buy', 'Strong Buy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf = np.array(conf)\n",
    "for i in range(4):\n",
    "    print(\"Label %d Precision: %.2f%%\" % (i, conf[i,i] * 100.0 / sum(conf[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matthews_corrcoef(np.argmax(val_Y, axis=-1), predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BI_DIR_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = BI_DIR_GRU(clusters=4)\n",
    "model = load_model('BI_DIR_GRU.h5')\n",
    "model.fit(train_X, train_Y,\n",
    "          batch_size=128,\n",
    "          epochs=6,\n",
    "          validation_data=(val_X, val_Y))\n",
    "model.save('BI_DIR_GRU.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate BI_DIR_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(model.predict(val_X), axis=-1)\n",
    "conf = confusion_matrix(np.argmax(val_Y, axis=-1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(val_Y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf,\n",
    "             index = [i for i in ['Strong Sell', 'Sell', 'Buy', 'Strong Buy'] ],\n",
    "             columns = [i for i in ['Strong Sell', 'Sell', 'Buy', 'Strong Buy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf = np.array(conf)\n",
    "for i in range(4):\n",
    "    print(\"Label %d Precision: %.2f%%\" % (i, conf[i,i] * 100.0 / sum(conf[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(np.argmax(val_Y, axis=-1), predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''U.S. judge rules Qualcomm owes Apple nearly $1 billion rebate payment\n",
    "\n",
    "A U.S. federal judge has issued a preliminary ruling that Qualcomm Inc owes Apple Inc nearly $1 billion in patent royalty rebate payments, though the decision is unlikely to result in Qualcomm writing a check to Apple because of other developments in the dispute.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''France's Monoprix working on expanding grocery alliance with Amazon\n",
    "\n",
    "PARIS Casino's upmarket Monoprix supermarket chain is working to expand its partnership with E-commerce giant Amazon in France, following a successful launch in Paris, Monoprix's Chief Executive said on Thursday.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''Microsoft workers demand it drop $480 million U.S. Army contract\n",
    "\n",
    "SAN FRANCISCO Some Microsoft Corp employees on Friday demanded that the company cancel a $480 million hardware contract to supply the U.S. Army, with 94 workers signing a petition calling on the company to stop developing \"any and all weapons technologies.\"\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "\n",
    "top_fin_words = pickle.load(open('reuters_top_fin_words.pkl', \"rb\"))\n",
    "\n",
    "test = [w for w in word_tokenize(test.lower()) if w in top_fin_words]\n",
    "test = pad_sequences([np.concatenate(tokenizer.texts_to_sequences(test), axis=0)], \n",
    "                     maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding='post')\n",
    "np.argmax(model.predict(test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
